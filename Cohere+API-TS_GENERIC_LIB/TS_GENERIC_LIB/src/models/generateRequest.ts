/**
 * Cohere APILib
 *
 * This file was automatically generated by APIMATIC v3.0 ( https://www.apimatic.io ).
 */

import {
  array,
  boolean,
  dict,
  number,
  object,
  optional,
  Schema,
  string,
} from '../schema';
import {
  ReturnLikelihoodsEnum,
  returnLikelihoodsEnumSchema,
} from './returnLikelihoodsEnum';
import { TruncateEnum, truncateEnumSchema } from './truncateEnum';

export interface GenerateRequest {
  /** Represents the prompt or text to be completed. Trailing whitespaces will be trimmed. If your use case requires trailing whitespaces, please contact ivan@cohere.ai. */
  prompt: string;
  /** The identifier of the model to generate with. Currently available models are `command` (default), `command-nightly` (experimental), `command-light`, and `command-light-nightly` (experimental). Smaller, "light" models are faster, while larger models will perform better. [Custom models](/docs/training-custom-models) can also be supplied with their full ID. */
  model?: string;
  /** Defaults to `1`, min value of `1`, max value of `5`. Denotes the maximum number of generations that will be returned. */
  numGenerations?: number;
  /**
   * Denotes the number of tokens to predict per generation, defaults to 20. See [BPE Tokens](/bpe-tokens-wiki) for more details.
   * Can only be set to `0` if `return_likelihoods` is set to `ALL` to get the likelihood of the prompt.
   */
  maxTokens?: number;
  /** The ID of a custom playground preset. You can create presets in the [playground](https://dashboard.cohere.ai/playground/generate?model=xlarge). If you use a preset, the `prompt` parameter becomes optional, and any included parameters will override the preset's parameters. */
  preset?: string;
  /** Defaults to `0.75`, min value of `0.0`, max value of `5.0`. A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations. See [Temperature](/temperature-wiki) for more details. */
  temperature?: number;
  /** Defaults to `0`(disabled), which is the minimum. Maximum value is `500`. Ensures only the top `k` most likely tokens are considered for generation at each step. */
  k?: number;
  /** Defaults to `0.75`. Set to `1.0` or `0` to disable. If set to a probability `0.0 < p < 1.0`, it ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step. If both `k` and `p` are enabled, `p` acts after `k`. */
  p?: number;
  /** Defaults to `0.0`, min value of `0.0`, max value of `1.0`. Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation. */
  frequencyPenalty?: number;
  /** Defaults to `0.0`, min value of `0.0`, max value of `1.0`. Can be used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies. */
  presencePenalty?: number;
  /** The generated text will be cut at the beginning of the earliest occurence of an end sequence. The sequence will be excluded from the text. */
  endSequences?: string[];
  /** The generated text will be cut at the end of the earliest occurence of a stop sequence. The sequence will be included the text. */
  stopSequences?: string[];
  returnLikelihoods?: ReturnLikelihoodsEnum;
  /**
   * Used to prevent the model from generating unwanted tokens or to incentivize it to include desired tokens. The format is `{token_id: bias}` where bias is a float between -10 and 10. Tokens can be obtained from text using [Tokenize](/reference/tokenize).
   * For example, if the value `{'11': -10}` is provided, the model will be very unlikely to include the token 11 (`"\n"`, the newline character) anywhere in the generated text. In contrast `{'11': 10}` will result in generations that nearly only contain that token. Values between -10 and 10 will proportionally affect the likelihood of the token appearing in the generated text.
   * Note: logit bias may not be supported for all custom models.
   */
  logitBias?: Record<string, number>;
  truncate?: TruncateEnum;
  /** When `true` the response will be streamed using JSON streaming. Default is `false`. */
  stream?: boolean;
}

export const generateRequestSchema: Schema<GenerateRequest> = object({
  prompt: ['prompt', string()],
  model: ['model', optional(string())],
  numGenerations: ['num_generations', optional(number())],
  maxTokens: ['max_tokens', optional(number())],
  preset: ['preset', optional(string())],
  temperature: ['temperature', optional(number())],
  k: ['k', optional(number())],
  p: ['p', optional(number())],
  frequencyPenalty: ['frequency_penalty', optional(number())],
  presencePenalty: ['presence_penalty', optional(number())],
  endSequences: ['end_sequences', optional(array(string()))],
  stopSequences: ['stop_sequences', optional(array(string()))],
  returnLikelihoods: [
    'return_likelihoods',
    optional(returnLikelihoodsEnumSchema),
  ],
  logitBias: ['logit_bias', optional(dict(number()))],
  truncate: ['truncate', optional(truncateEnumSchema)],
  stream: ['stream', optional(boolean())],
});
